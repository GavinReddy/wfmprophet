{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: 7-Day Call Volume Forecast with Prophet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deep Dive into Facebook Prophet for Contact Centre Workforce Planning\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Prophet for This Problem?\n",
    "\n",
    "Prophet was designed by Facebook's Core Data Science team specifically for the class of problem you face every Monday morning: business time series with strong seasonal patterns, known holiday effects, and the need for analyst-interpretable forecasts that non-technical stakeholders can trust.\n",
    "\n",
    "For daily call volume forecasting at a retail bank, Prophet's architecture maps almost perfectly onto the data-generating process:\n",
    "\n",
    "- **Piecewise linear or logistic growth trend** → captures the slow drift in call volume as customers migrate to digital channels\n",
    "- **Fourier-based seasonality** → handles both weekly and annual cycles simultaneously (something SARIMA cannot do natively)\n",
    "- **Built-in holiday framework** → models the exact effect of Irish bank holidays, including the post-holiday rebound\n",
    "- **Custom regressors** → incorporates banking-specific drivers like direct debit dates and marketing campaigns\n",
    "- **Automatic changepoint detection** → adapts to structural shifts (branch closures, new IVR routing, COVID-era changes)\n",
    "- **Uncertainty intervals** → feeds directly into risk-adjusted staffing decisions\n",
    "\n",
    "This case study works through the full end-to-end workflow with Prophet as the primary model, showing every line of code you need to go from raw data to a production-ready 7-day forecast.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: EXPLORE\n",
    "\n",
    "### 1.1 Load and Prepare the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Load data\n",
    "# ──────────────────────────────────────────────────\n",
    "# Your data should have at minimum: date, daily_call_volume\n",
    "# Additional columns (if available): channel, queue, call_type\n",
    "df = pd.read_csv('daily_call_volume.csv', parse_dates=['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f'Date range : {df[\"date\"].min()} to {df[\"date\"].max()}')\n",
    "print(f'Observations: {len(df)}')\n",
    "print(f'Missing dates: {pd.date_range(df[\"date\"].min(), df[\"date\"].max()).difference(df[\"date\"]).shape[0]}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "print(df.head())\n",
    "print(f'\\nDescriptive statistics:')\n",
    "print(df['calls'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot the Full History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "ax.plot(df['date'], df['calls'], linewidth=0.5, color='#2c3e50')\n",
    "ax.set_title('Daily Call Volume — Full History', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Calls')\n",
    "ax.set_xlabel(None)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you are documenting at this stage:**\n",
    "\n",
    "Read the plot systematically — trend direction, seasonal patterns (weekly sawtooth, annual wave), variance behaviour (constant or growing), and any visible anomalies (spikes, drops, flat periods). Write down your observations before running any statistical tests. The visual inspection guides every decision that follows.\n",
    "\n",
    "### 1.3 Weekly Pattern Deep Dive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=df, x='day_of_week', y='calls', order=dow_order,\n",
    "            palette='coolwarm', ax=axes[0])\n",
    "axes[0].set_title('Volume Distribution by Day of Week', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel(None)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Mean with confidence interval\n",
    "dow_means = df.groupby('day_of_week')['calls'].agg(['mean', 'std', 'count']).reindex(dow_order)\n",
    "dow_means['se'] = dow_means['std'] / np.sqrt(dow_means['count'])\n",
    "axes[1].bar(range(7), dow_means['mean'], yerr=1.96 * dow_means['se'],\n",
    "            color='#3498db', capsize=4, alpha=0.8)\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "axes[1].set_title('Mean Volume ± 95% CI by Day of Week', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Calls')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the day-of-week profile as percentages (useful later for validation)\n",
    "dow_pct = dow_means['mean'] / dow_means['mean'].sum() * 100\n",
    "print('Day-of-week profile (% of weekly volume):')\n",
    "for day, pct in dow_pct.items():\n",
    "    print(f'  {day:12s}: {pct:.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Monthly and Annual Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Aggregate to weekly to smooth out day-of-week noise before looking at annual pattern\n",
    "weekly = df.set_index('date')['calls'].resample('W').mean().reset_index()\n",
    "weekly['month'] = weekly['date'].dt.month\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.boxplot(data=weekly, x='month', y='calls', palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('Weekly-Averaged Volume by Month', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Avg Daily Calls')\n",
    "\n",
    "# Year-over-year overlay (seasonal plot)\n",
    "for year, group in df.groupby('year'):\n",
    "    group_weekly = group.set_index('date')['calls'].resample('W').mean()\n",
    "    axes[1].plot(range(len(group_weekly)), group_weekly.values, label=str(year), alpha=0.7)\n",
    "axes[1].set_title('Year-over-Year Weekly Average', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Week of Year')\n",
    "axes[1].set_ylabel('Avg Daily Calls')\n",
    "axes[1].legend(title='Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up as a properly indexed series for MSTL\n",
    "ts = df.set_index('date')['calls'].asfreq('D')\n",
    "ts = ts.interpolate(method='linear')  # Fill any gaps for decomposition\n",
    "\n",
    "# MSTL: weekly (7) and annual (365) periods\n",
    "mstl = MSTL(ts, periods=[7, 365], stl_kwargs={'robust': True})\n",
    "result = mstl.fit()\n",
    "\n",
    "fig = result.plot()\n",
    "fig.set_size_inches(16, 12)\n",
    "fig.suptitle('MSTL Decomposition (Weekly + Annual Seasonality)', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantify strength\n",
    "var_resid = np.var(result.resid)\n",
    "print(f'Strength of trend           : {max(0, 1 - var_resid / np.var(result.resid + result.trend - result.trend.mean())):.3f}')\n",
    "for col in result.seasonal.columns:\n",
    "    F_s = max(0, 1 - var_resid / np.var(result.resid + result.seasonal[col]))\n",
    "    print(f'Strength of seasonality ({col}): {F_s:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 ACF/PACF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plot_acf(ts.dropna(), lags=60, ax=axes[0], title='ACF: Daily Call Volume')\n",
    "plot_pacf(ts.dropna(), lags=60, method='ywm', ax=axes[1], title='PACF: Daily Call Volume')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected signatures:** Spikes at lags 7, 14, 21, 28 in the ACF (weekly cycle). Strong PACF spikes at lags 1 and 7. Slow ACF decay if there is a trend component.\n",
    "\n",
    "### 1.7 Stationarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_result = adfuller(ts.dropna(), autolag='AIC')\n",
    "kpss_result = kpss(ts.dropna(), regression='ct', nlags='auto')\n",
    "\n",
    "print(f'ADF  — statistic: {adf_result[0]:.4f}, p-value: {adf_result[1]:.4f}')\n",
    "print(f'KPSS — statistic: {kpss_result[0]:.4f}, p-value: {kpss_result[1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for Prophet users:** Prophet does not require stationarity. It models trend and seasonality explicitly as additive components, so you do not need to difference the data. However, understanding whether the series is stationary still informs your choice of `growth` parameter (linear vs logistic vs flat) and whether changepoints are likely.\n",
    "\n",
    "### 1.8 EDA Summary\n",
    "\n",
    "| Aspect | Typical Finding for Bank Call Data |\n",
    "|---|---|\n",
    "| Primary seasonality | Weekly (period=7), very strong |\n",
    "| Secondary seasonality | Annual (period≈365), moderate |\n",
    "| Trend | Slight downward (digital migration) or flat |\n",
    "| Variance | Roughly constant (additive seasonality likely) |\n",
    "| Key external drivers | Bank holidays, direct debit dates, campaigns, outages |\n",
    "| ACF signature | Strong at lags 7, 14, 21; also lag 1 |\n",
    "| Stationarity | Often stationary or trend-stationary |\n",
    "| Outliers | Holiday rebounds, system incidents, COVID period |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: PREPROCESS\n",
    "\n",
    "### 2.1 Prepare the Prophet DataFrame\n",
    "\n",
    "Prophet requires a DataFrame with exactly two columns: `ds` (datestamp) and `y` (target). Additional columns are regressors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Core DataFrame\n",
    "# ──────────────────────────────────────────────────\n",
    "prophet_df = df[['date', 'calls']].copy()\n",
    "prophet_df = prophet_df.rename(columns={'date': 'ds', 'calls': 'y'})\n",
    "\n",
    "# Ensure continuous daily index (Prophet can handle missing values but\n",
    "# performs better with a complete index)\n",
    "full_dates = pd.DataFrame({'ds': pd.date_range(prophet_df['ds'].min(), prophet_df['ds'].max(), freq='D')})\n",
    "prophet_df = full_dates.merge(prophet_df, on='ds', how='left')\n",
    "\n",
    "# Check for missing values\n",
    "n_missing = prophet_df['y'].isna().sum()\n",
    "print(f'Missing values: {n_missing}')\n",
    "if n_missing > 0:\n",
    "    print('Filling with linear interpolation (acceptable for small gaps)')\n",
    "    prophet_df['y'] = prophet_df['y'].interpolate(method='linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handle Outliers\n",
    "\n",
    "Prophet is sensitive to outliers — a single anomalous day can distort the trend changepoints. The recommended approach is to set outlier values to `NaN` and let Prophet impute them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Flag and remove data quality outliers\n",
    "# (NOT holiday effects — those are modelled explicitly)\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "# Method: IQR within each day-of-week group\n",
    "def flag_outliers(group, k=3.0):\n",
    "    q1, q3 = group['y'].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    group['is_outlier'] = (group['y'] < q1 - k * iqr) | (group['y'] > q3 + k * iqr)\n",
    "    return group\n",
    "\n",
    "prophet_df['dow'] = prophet_df['ds'].dt.dayofweek\n",
    "prophet_df = prophet_df.groupby('dow', group_keys=False).apply(flag_outliers)\n",
    "\n",
    "n_outliers = prophet_df['is_outlier'].sum()\n",
    "print(f'Flagged {n_outliers} outliers ({n_outliers / len(prophet_df) * 100:.1f}%)')\n",
    "\n",
    "# Review flagged outliers\n",
    "print(prophet_df[prophet_df['is_outlier']][['ds', 'y']].head(20))\n",
    "\n",
    "# Set outliers to NaN — Prophet handles NaN by imputing during fitting\n",
    "# IMPORTANT: Only do this for genuine data quality issues, not for\n",
    "# explainable events (holidays, campaigns). Those should be modelled.\n",
    "prophet_df.loc[prophet_df['is_outlier'], 'y'] = np.nan\n",
    "\n",
    "# Clean up helper columns\n",
    "prophet_df = prophet_df.drop(columns=['dow', 'is_outlier'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build the Holiday DataFrame\n",
    "\n",
    "Prophet's holiday framework models each holiday as a separate effect with configurable windows before and after:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Irish Bank Holidays\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "# Define Irish bank holidays for the full date range\n",
    "# Source: https://www.gov.ie/en/publication/public-holidays/\n",
    "def get_irish_bank_holidays(start_year, end_year):\n",
    "    \"\"\"Generate Irish public holidays for the given year range.\"\"\"\n",
    "    holidays = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Fixed-date holidays\n",
    "        holidays.append({'ds': f'{year}-01-01', 'holiday': \"New Year's Day\"})\n",
    "        holidays.append({'ds': f'{year}-02-01', 'holiday': \"St Brigid's Day\"})\n",
    "        holidays.append({'ds': f'{year}-03-17', 'holiday': \"St Patrick's Day\"})\n",
    "        holidays.append({'ds': f'{year}-05-06', 'holiday': 'May Bank Holiday'})  # First Mon in May (approximate)\n",
    "        holidays.append({'ds': f'{year}-06-03', 'holiday': 'June Bank Holiday'})  # First Mon in June (approximate)\n",
    "        holidays.append({'ds': f'{year}-08-05', 'holiday': 'August Bank Holiday'})  # First Mon in August (approximate)\n",
    "        holidays.append({'ds': f'{year}-10-28', 'holiday': 'October Bank Holiday'})  # Last Mon in October (approximate)\n",
    "        holidays.append({'ds': f'{year}-12-25', 'holiday': 'Christmas Day'})\n",
    "        holidays.append({'ds': f'{year}-12-26', 'holiday': \"St Stephen's Day\"})\n",
    "\n",
    "        # Easter (variable) — use a library for accurate dates\n",
    "        # pip install dateutil\n",
    "        from dateutil.easter import easter\n",
    "        easter_date = easter(year)\n",
    "        holidays.append({'ds': str(easter_date - pd.Timedelta(days=2)), 'holiday': 'Good Friday'})\n",
    "        holidays.append({'ds': str(easter_date + pd.Timedelta(days=1)), 'holiday': 'Easter Monday'})\n",
    "\n",
    "    return pd.DataFrame(holidays)\n",
    "\n",
    "holidays_df = get_irish_bank_holidays(2022, 2027)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Configure holiday windows\n",
    "# ──────────────────────────────────────────────────\n",
    "# lower_window = days BEFORE the holiday that are affected\n",
    "# upper_window = days AFTER the holiday that are affected\n",
    "\n",
    "# The day AFTER a bank holiday is often the highest-volume day of the month\n",
    "# because callers who couldn't get through on the holiday call the next morning.\n",
    "# This rebound effect is the single most important holiday feature.\n",
    "\n",
    "# Standard bank holidays: quiet on the day, spike the day after\n",
    "standard_holidays = holidays_df[~holidays_df['holiday'].isin(['Christmas Day', \"St Stephen's Day\"])].copy()\n",
    "standard_holidays['lower_window'] = 0   # No effect before\n",
    "standard_holidays['upper_window'] = 1   # Rebound the day after\n",
    "\n",
    "# Christmas period: extended closure with multi-day rebound\n",
    "christmas = holidays_df[holidays_df['holiday'].isin(['Christmas Day', \"St Stephen's Day\"])].copy()\n",
    "christmas_day = christmas[christmas['holiday'] == 'Christmas Day'].copy()\n",
    "christmas_day['lower_window'] = -1  # Christmas Eve often sees reduced volume\n",
    "christmas_day['upper_window'] = 4   # Takes several days to clear the backlog\n",
    "\n",
    "# Combine\n",
    "holidays_final = pd.concat([standard_holidays, christmas_day], ignore_index=True)\n",
    "\n",
    "print(f'Holiday events defined: {len(holidays_final)}')\n",
    "print(holidays_final.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build Custom Regressors\n",
    "\n",
    "These capture effects that Prophet's seasonality and holidays cannot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Banking-specific regressors\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "def add_banking_regressors(df):\n",
    "    \"\"\"Add domain-specific features to the Prophet DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Direct debit processing dates\n",
    "    # The 1st and 15th of each month generate failed-payment calls\n",
    "    df['is_direct_debit'] = df['ds'].dt.day.isin([1, 2, 15, 16]).astype(float)\n",
    "\n",
    "    # Payday window (most Irish salaries paid last working day of month or 15th)\n",
    "    df['is_payday_window'] = ((df['ds'].dt.day >= 25) | (df['ds'].dt.day <= 3)).astype(float)\n",
    "\n",
    "    # Month-end processing (statement queries, fee disputes)\n",
    "    df['is_month_end'] = (df['ds'].dt.day >= 28).astype(float)\n",
    "\n",
    "    # January effect (New Year financial resolutions, tax prep)\n",
    "    df['is_january'] = (df['ds'].dt.month == 1).astype(float)\n",
    "\n",
    "    # Tax deadline periods (Irish self-assessment: Oct 31, preliminary tax: Nov)\n",
    "    df['is_tax_season'] = df['ds'].dt.month.isin([10, 11]).astype(float)\n",
    "\n",
    "    # Weekend flag (if centre operates 7 days but with reduced weekend hours)\n",
    "    df['is_weekend'] = (df['ds'].dt.dayofweek >= 5).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "prophet_df = add_banking_regressors(prophet_df)\n",
    "\n",
    "# Verify\n",
    "print('Regressor columns:')\n",
    "print([col for col in prophet_df.columns if col not in ['ds', 'y']])\n",
    "print(f'\\nSample:')\n",
    "print(prophet_df[['ds', 'y', 'is_direct_debit', 'is_payday_window', 'is_weekend']].tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Hold out the most recent 8 weeks for evaluation\n",
    "# This gives 8 non-overlapping 7-day forecast windows\n",
    "# ──────────────────────────────────────────────────\n",
    "FORECAST_HORIZON = 7\n",
    "N_TEST_WEEKS = 8\n",
    "TEST_DAYS = N_TEST_WEEKS * FORECAST_HORIZON\n",
    "\n",
    "cutoff_date = prophet_df['ds'].max() - pd.Timedelta(days=TEST_DAYS - 1)\n",
    "train = prophet_df[prophet_df['ds'] < cutoff_date].copy()\n",
    "test = prophet_df[prophet_df['ds'] >= cutoff_date].copy()\n",
    "\n",
    "print(f'Train: {train[\"ds\"].min().date()} to {train[\"ds\"].max().date()} ({len(train)} days)')\n",
    "print(f'Test : {test[\"ds\"].min().date()} to {test[\"ds\"].max().date()} ({len(test)} days)')\n",
    "print(f'Test weeks: {N_TEST_WEEKS}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: MODEL\n",
    "\n",
    "### 3.1 Baseline — Seasonal Naïve\n",
    "\n",
    "Always establish the baseline before fitting Prophet. This is the benchmark that Prophet must beat to be worth deploying:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Seasonal naïve: same day last week\n",
    "# ──────────────────────────────────────────────────\n",
    "test_with_baseline = test.copy()\n",
    "test_with_baseline['naive_lag7'] = prophet_df.set_index('ds')['y'].shift(7).loc[test['ds']].values\n",
    "\n",
    "# 4-week same-day average\n",
    "for i, row in test_with_baseline.iterrows():\n",
    "    same_dow_dates = [row['ds'] - pd.Timedelta(weeks=w) for w in range(1, 5)]\n",
    "    same_dow_vals = prophet_df[prophet_df['ds'].isin(same_dow_dates)]['y']\n",
    "    test_with_baseline.loc[i, 'naive_avg4w'] = same_dow_vals.mean()\n",
    "\n",
    "baseline_mae = np.abs(test_with_baseline['y'] - test_with_baseline['naive_lag7']).mean()\n",
    "baseline_mape = (np.abs(test_with_baseline['y'] - test_with_baseline['naive_lag7']) / test_with_baseline['y']).mean() * 100\n",
    "print(f'Seasonal Naïve Baseline — MAE: {baseline_mae:.0f}, MAPE: {baseline_mape:.1f}%')\n",
    "\n",
    "avg4w_mae = np.abs(test_with_baseline['y'] - test_with_baseline['naive_avg4w']).mean()\n",
    "avg4w_mape = (np.abs(test_with_baseline['y'] - test_with_baseline['naive_avg4w']) / test_with_baseline['y']).mean() * 100\n",
    "print(f'4-Week Avg Baseline    — MAE: {avg4w_mae:.0f}, MAPE: {avg4w_mape:.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prophet — Configuration and Fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Configure the Prophet model\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "model = Prophet(\n",
    "    # ── Trend ──\n",
    "    growth='linear',                  # Linear trend (use 'logistic' if volume has a ceiling)\n",
    "    changepoint_prior_scale=0.05,     # Controls trend flexibility\n",
    "                                      # Lower = smoother trend, less overfitting\n",
    "                                      # Higher = more responsive to level shifts\n",
    "                                      # Default 0.05 is good; tune between 0.001 and 0.5\n",
    "    changepoint_range=0.9,            # Look for changepoints in first 90% of data\n",
    "                                      # (default 0.8 — extending to 0.9 helps if\n",
    "                                      # recent trend shifts are important)\n",
    "    n_changepoints=25,                # Number of potential changepoint locations\n",
    "\n",
    "    # ── Seasonality ──\n",
    "    yearly_seasonality=True,          # Annual cycle (Fourier order auto-selected)\n",
    "    weekly_seasonality=True,          # Weekly cycle (default Fourier order = 3)\n",
    "    daily_seasonality=False,          # Not applicable for daily-level data\n",
    "\n",
    "    seasonality_mode='additive',      # IMPORTANT DECISION\n",
    "                                      # Use 'additive' if seasonal swings are constant\n",
    "                                      # in absolute terms regardless of the level.\n",
    "                                      # Use 'multiplicative' if swings grow proportionally.\n",
    "                                      # For call centre data, 'additive' is usually correct.\n",
    "                                      # Validate by checking STL residuals (Stage 1).\n",
    "\n",
    "    seasonality_prior_scale=10.0,     # Regularisation on seasonal components\n",
    "                                      # Lower = smoother seasonality, less overfitting\n",
    "                                      # Higher = allows sharper seasonal patterns\n",
    "                                      # Default 10 is usually fine for weekly/yearly\n",
    "\n",
    "    # ── Holidays ──\n",
    "    holidays=holidays_final,\n",
    "    holidays_prior_scale=10.0,        # Regularisation on holiday effects\n",
    "                                      # Lower = shrink holiday effects toward zero\n",
    "                                      # Higher = allow large holiday spikes/drops\n",
    "                                      # For call centres, holidays have BIG effects,\n",
    "                                      # so keep this at 10+ or even increase to 20\n",
    "\n",
    "    # ── Uncertainty ──\n",
    "    interval_width=0.80,              # 80% prediction intervals\n",
    "    mcmc_samples=0,                   # 0 = MAP estimation (fast)\n",
    "                                      # 300+ = full Bayesian (slow but better intervals)\n",
    ")\n",
    "\n",
    "# ── Add custom regressors ──\n",
    "# Each regressor gets its own regularisation prior\n",
    "model.add_regressor('is_direct_debit', prior_scale=10.0, mode='additive')\n",
    "model.add_regressor('is_payday_window', prior_scale=5.0, mode='additive')\n",
    "model.add_regressor('is_month_end', prior_scale=5.0, mode='additive')\n",
    "model.add_regressor('is_january', prior_scale=5.0, mode='additive')\n",
    "model.add_regressor('is_tax_season', prior_scale=5.0, mode='additive')\n",
    "model.add_regressor('is_weekend', prior_scale=15.0, mode='additive')  # Strong known effect\n",
    "\n",
    "# ── Optional: add country-level holidays as a catch-all ──\n",
    "# This adds Prophet's built-in Irish holiday calendar ON TOP of your custom holidays.\n",
    "# It can catch holidays you missed, but may also double-count.\n",
    "# Test with and without to see which performs better.\n",
    "# model.add_country_holidays(country_name='IE')\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Fit the model\n",
    "# ──────────────────────────────────────────────────\n",
    "model.fit(train)\n",
    "print('Model fitted successfully.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Understanding What Prophet Learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Inspect the components\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "# Generate in-sample predictions\n",
    "train_forecast = model.predict(train)\n",
    "\n",
    "# Plot all components\n",
    "fig = model.plot_components(train_forecast)\n",
    "fig.set_size_inches(14, 16)\n",
    "plt.suptitle('Prophet Components', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the component plots:**\n",
    "\n",
    "- **Trend panel:** Shows the piecewise linear trend with changepoints. Are the changepoints at sensible dates? Do they correspond to known events (branch closures, system changes, COVID)? If the trend looks too wiggly, reduce `changepoint_prior_scale`.\n",
    "\n",
    "- **Weekly panel:** Shows the day-of-week effect. Monday should be highest, weekend lowest. Does the shape match your box plots from Stage 1? If not, increase `weekly_seasonality` Fourier order.\n",
    "\n",
    "- **Yearly panel:** Shows the annual cycle. January and October (tax season) should be high; August should be low. If this looks too smooth and misses known seasonal features, increase the yearly Fourier order.\n",
    "\n",
    "- **Holiday panels:** Shows the estimated effect of each holiday. Christmas should show a large negative effect (closure) followed by positive rebounds.\n",
    "\n",
    "- **Regressor panels:** Shows the estimated effect of each custom regressor. `is_direct_debit` should be positive (more calls on DD days). `is_weekend` should be strongly negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Inspect changepoints\n",
    "# ──────────────────────────────────────────────────\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "\n",
    "fig = model.plot(train_forecast)\n",
    "add_changepoints_to_plot(fig.gca(), model, train_forecast)\n",
    "fig.set_size_inches(18, 6)\n",
    "plt.title('Fitted Values with Changepoints (red dashed lines)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print changepoint dates and magnitudes\n",
    "changepoints = model.changepoints\n",
    "deltas = model.params['delta'].mean(axis=0)\n",
    "significant_cps = pd.DataFrame({\n",
    "    'date': changepoints,\n",
    "    'delta': deltas\n",
    "}).sort_values('delta', key=abs, ascending=False)\n",
    "\n",
    "print('Top 10 changepoints by magnitude:')\n",
    "print(significant_cps.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generate the 7-Day Forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Create the future DataFrame\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "# Prophet's make_future_dataframe generates dates, but we need to\n",
    "# populate the regressor columns for the forecast period\n",
    "\n",
    "future = model.make_future_dataframe(periods=FORECAST_HORIZON, freq='D')\n",
    "\n",
    "# Add the same regressors to the future DataFrame\n",
    "# CRITICAL: These must be populated with KNOWN values for the future dates.\n",
    "# Calendar-based features are deterministic — you know what day of the week\n",
    "# next Tuesday is, and whether the 1st falls next week.\n",
    "future = add_banking_regressors(future)\n",
    "\n",
    "# Sanity check: no NaN in regressors for future dates\n",
    "future_only = future[future['ds'] > train['ds'].max()]\n",
    "print('Future dates:')\n",
    "print(future_only[['ds', 'is_direct_debit', 'is_payday_window', 'is_weekend']])\n",
    "assert future_only[['is_direct_debit', 'is_payday_window', 'is_month_end',\n",
    "                     'is_january', 'is_tax_season', 'is_weekend']].isna().sum().sum() == 0, \\\n",
    "    'ERROR: NaN values in future regressors!'\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Generate forecast\n",
    "# ──────────────────────────────────────────────────\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Extract the 7-day forecast\n",
    "forecast_7d = forecast[forecast['ds'] > train['ds'].max()].copy()\n",
    "forecast_7d = forecast_7d[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].reset_index(drop=True)\n",
    "\n",
    "print('\\n7-Day Forecast:')\n",
    "print('─' * 65)\n",
    "print(f'{\"Date\":<14} {\"Day\":<12} {\"Forecast\":>10} {\"Lower 80%\":>12} {\"Upper 80%\":>12}')\n",
    "print('─' * 65)\n",
    "for _, row in forecast_7d.iterrows():\n",
    "    day_name = row['ds'].strftime('%A')\n",
    "    print(f'{row[\"ds\"].strftime(\"%Y-%m-%d\"):<14} {day_name:<12} '\n",
    "          f'{row[\"yhat\"]:>10,.0f} {row[\"yhat_lower\"]:>12,.0f} {row[\"yhat_upper\"]:>12,.0f}')\n",
    "print('─' * 65)\n",
    "print(f'{\"Weekly Total\":<26} {forecast_7d[\"yhat\"].sum():>10,.0f} '\n",
    "      f'{forecast_7d[\"yhat_lower\"].sum():>12,.0f} {forecast_7d[\"yhat_upper\"].sum():>12,.0f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualise the Forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Plot: recent history + forecast\n",
    "# ──────────────────────────────────────────────────\n",
    "lookback_days = 60\n",
    "recent = forecast[forecast['ds'] >= (train['ds'].max() - pd.Timedelta(days=lookback_days))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Historical actuals\n",
    "hist_mask = recent['ds'] <= train['ds'].max()\n",
    "ax.plot(recent.loc[hist_mask, 'ds'], train.set_index('ds').loc[recent.loc[hist_mask, 'ds'], 'y'],\n",
    "        color='#2c3e50', linewidth=1, label='Actual', marker='o', markersize=2)\n",
    "\n",
    "# Prophet fitted values (recent history)\n",
    "ax.plot(recent.loc[hist_mask, 'ds'], recent.loc[hist_mask, 'yhat'],\n",
    "        color='#3498db', linewidth=1, alpha=0.5, label='Prophet Fitted')\n",
    "\n",
    "# Forecast\n",
    "forecast_mask = recent['ds'] > train['ds'].max()\n",
    "ax.plot(recent.loc[forecast_mask, 'ds'], recent.loc[forecast_mask, 'yhat'],\n",
    "        color='#e74c3c', linewidth=2, marker='o', markersize=6, label='7-Day Forecast')\n",
    "ax.fill_between(recent.loc[forecast_mask, 'ds'],\n",
    "                recent.loc[forecast_mask, 'yhat_lower'],\n",
    "                recent.loc[forecast_mask, 'yhat_upper'],\n",
    "                color='#e74c3c', alpha=0.15, label='80% Prediction Interval')\n",
    "\n",
    "# Cutoff line\n",
    "ax.axvline(x=train['ds'].max(), color='grey', linestyle='--', linewidth=1, alpha=0.7, label='Forecast Origin')\n",
    "\n",
    "ax.set_title('Daily Call Volume: Recent History + 7-Day Prophet Forecast', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Calls')\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: DIAGNOSE\n",
    "\n",
    "### 4.1 In-Sample Residual Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Compute residuals\n",
    "# ──────────────────────────────────────────────────\n",
    "train_with_fit = train.merge(\n",
    "    train_forecast[['ds', 'yhat']], on='ds', how='left'\n",
    ")\n",
    "train_with_fit['residual'] = train_with_fit['y'] - train_with_fit['yhat']\n",
    "train_with_fit['pct_error'] = train_with_fit['residual'] / train_with_fit['y'] * 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Residuals over time\n",
    "axes[0, 0].plot(train_with_fit['ds'], train_with_fit['residual'], linewidth=0.4, color='#2c3e50')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=0.8)\n",
    "axes[0, 0].set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residual (calls)')\n",
    "\n",
    "# 2. Histogram\n",
    "axes[0, 1].hist(train_with_fit['residual'].dropna(), bins=50, color='#3498db', edgecolor='white')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[0, 1].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. QQ plot\n",
    "from scipy import stats\n",
    "stats.probplot(train_with_fit['residual'].dropna(), plot=axes[0, 2])\n",
    "axes[0, 2].set_title('QQ Plot', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. ACF of residuals\n",
    "plot_acf(train_with_fit['residual'].dropna(), lags=35, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('ACF of Residuals', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 5. Residuals by day of week\n",
    "train_with_fit['dow'] = train_with_fit['ds'].dt.day_name()\n",
    "sns.boxplot(data=train_with_fit, x='dow', y='residual',\n",
    "            order=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n",
    "            ax=axes[1, 1])\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1, 1].set_title('Residuals by Day of Week', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Residuals vs fitted (check for heteroscedasticity)\n",
    "axes[1, 2].scatter(train_with_fit['yhat'], train_with_fit['residual'],\n",
    "                   alpha=0.2, s=5, color='#2c3e50')\n",
    "axes[1, 2].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1, 2].set_title('Residuals vs Fitted', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Fitted Value')\n",
    "axes[1, 2].set_ylabel('Residual')\n",
    "\n",
    "plt.suptitle('Prophet Residual Diagnostics', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnostic checklist — what to look for and what to do about it:**\n",
    "\n",
    "| Observation | Diagnosis | Action |\n",
    "|---|---|---|\n",
    "| Residuals have significant ACF at lag 7 | Prophet's weekly seasonality is insufficient | Increase `weekly_seasonality` Fourier order: `model = Prophet(weekly_seasonality=10)` instead of default 3 |\n",
    "| Residuals have significant ACF at lag 1 | Short-term autocorrelation not captured | Prophet fundamentally does not model autocorrelation. Consider post-processing residuals with an AR(1) model, or switch to a hybrid approach |\n",
    "| Residual box plots show Monday bias (positive) | Prophet underpredicts Mondays | Add a specific `is_monday` regressor, or increase Fourier order for weekly seasonality |\n",
    "| Funnel shape in residuals-vs-fitted | Heteroscedasticity | Switch to `seasonality_mode='multiplicative'` or apply log transform to `y` |\n",
    "| Residuals spike on specific dates | Missed holiday or event | Add the event to the holidays DataFrame |\n",
    "| Residual variance increases over time | Model drift | Retrain more frequently or reduce `changepoint_range` |\n",
    "\n",
    "### 4.2 Ljung-Box Test on Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "lb = acorr_ljungbox(train_with_fit['residual'].dropna(), lags=[7, 14, 21, 28], return_df=True)\n",
    "print('Ljung-Box Test (H₀: no autocorrelation in residuals):')\n",
    "print(lb)\n",
    "print()\n",
    "for lag, row in lb.iterrows():\n",
    "    status = '✅ No autocorrelation' if row['lb_pvalue'] > 0.05 else '❌ Autocorrelation remains'\n",
    "    print(f'  Lag {lag:2d}: p={row[\"lb_pvalue\"]:.4f}  {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the Ljung-Box test fails (common with Prophet):** Prophet treats errors as independent, which means it does not model the autoregressive structure that remains in residuals. This is Prophet's main theoretical limitation. Options:\n",
    "\n",
    "1. **Accept it** — for a 7-day horizon, the impact on forecast accuracy is often modest\n",
    "2. **Post-process with an ARIMA on residuals** — fit an AR model to Prophet's residuals and add the AR forecast to Prophet's forecast (hybrid approach)\n",
    "3. **Switch to a model that captures autocorrelation** — XGBoost with lag features, or SARIMA\n",
    "\n",
    "### 4.3 Prophet Cross-Validation\n",
    "\n",
    "Prophet has a built-in cross-validation framework that simulates the rolling-origin evaluation you would do in production:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Cross-validation setup\n",
    "# ──────────────────────────────────────────────────\n",
    "# initial  : minimum training data before first forecast\n",
    "# period   : spacing between forecast origins (every 7 days = weekly)\n",
    "# horizon  : how far ahead to forecast (7 days)\n",
    "\n",
    "cv_results = cross_validation(\n",
    "    model,\n",
    "    initial='730 days',   # Use at least 2 years of training data\n",
    "    period='7 days',      # Make a new forecast every week\n",
    "    horizon='7 days',     # Forecast 7 days ahead each time\n",
    "    parallel='processes'  # Speed up with multiprocessing\n",
    ")\n",
    "\n",
    "print(f'Cross-validation produced {len(cv_results)} forecast-actual pairs')\n",
    "print(f'Across {cv_results[\"cutoff\"].nunique()} forecast origins')\n",
    "print(cv_results.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Performance metrics\n",
    "# ──────────────────────────────────────────────────\n",
    "metrics = performance_metrics(cv_results, rolling_window=1)\n",
    "print(metrics[['horizon', 'mae', 'mape', 'rmse', 'coverage']].to_string(index=False))\n",
    "\n",
    "# Overall performance\n",
    "print(f'\\nOverall CV Performance:')\n",
    "print(f'  MAE     : {metrics[\"mae\"].mean():.0f} calls')\n",
    "print(f'  MAPE    : {metrics[\"mape\"].mean() * 100:.1f}%')\n",
    "print(f'  RMSE    : {metrics[\"rmse\"].mean():.0f} calls')\n",
    "print(f'  Coverage: {metrics[\"coverage\"].mean() * 100:.1f}% (target: 80%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Performance by forecast horizon\n",
    "# ──────────────────────────────────────────────────\n",
    "fig = plot_cross_validation_metric(cv_results, metric='mape')\n",
    "fig.set_size_inches(14, 5)\n",
    "plt.title('MAPE by Forecast Horizon (days ahead)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Performance by day of week\n",
    "# ──────────────────────────────────────────────────\n",
    "cv_results['dow'] = cv_results['ds'].dt.day_name()\n",
    "cv_results['abs_pct_error'] = np.abs((cv_results['y'] - cv_results['yhat']) / cv_results['y'])\n",
    "\n",
    "dow_perf = cv_results.groupby('dow')['abs_pct_error'].agg(['mean', 'median', 'std']).reindex(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    ") * 100\n",
    "\n",
    "print('\\nMAPE by Day of Week:')\n",
    "print(dow_perf.round(1).rename(columns={'mean': 'Mean MAPE%', 'median': 'Median MAPE%', 'std': 'Std%'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Performance over time (detect drift)\n",
    "# ──────────────────────────────────────────────────\n",
    "cv_by_cutoff = cv_results.groupby('cutoff').agg(\n",
    "    mape=('abs_pct_error', 'mean'),\n",
    "    mae=('y', lambda x: np.abs(cv_results.loc[x.index, 'y'] - cv_results.loc[x.index, 'yhat']).mean())\n",
    ").reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.plot(cv_by_cutoff['cutoff'], cv_by_cutoff['mape'] * 100, marker='o', markersize=3, linewidth=0.8)\n",
    "ax.axhline(y=10, color='orange', linestyle='--', label='10% MAPE threshold')\n",
    "ax.set_title('Cross-Validation MAPE Over Time (each point = one 7-day forecast)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('MAPE (%)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Prediction Interval Calibration\n",
    "\n",
    "A forecast is only as useful as its uncertainty estimate. If the model says \"80% interval\", roughly 80% of actuals should fall within it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check empirical coverage\n",
    "cv_results['within_interval'] = (\n",
    "    (cv_results['y'] >= cv_results['yhat_lower']) &\n",
    "    (cv_results['y'] <= cv_results['yhat_upper'])\n",
    ")\n",
    "\n",
    "overall_coverage = cv_results['within_interval'].mean() * 100\n",
    "print(f'Empirical coverage: {overall_coverage:.1f}% (target: {model.interval_width * 100:.0f}%)')\n",
    "\n",
    "# Coverage by day of week\n",
    "dow_coverage = cv_results.groupby('dow')['within_interval'].mean().reindex(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    ") * 100\n",
    "print('\\nCoverage by Day of Week:')\n",
    "for day, cov in dow_coverage.items():\n",
    "    status = '✅' if abs(cov - 80) < 10 else '⚠️'\n",
    "    print(f'  {day:12s}: {cov:.1f}%  {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If coverage is too low (< 75%):** Intervals are too narrow. Increase `mcmc_samples` (300+) for better uncertainty estimation, or widen the `interval_width`. If coverage is consistently low on Mondays, the model is underestimating Monday variance.\n",
    "\n",
    "**If coverage is too high (> 90%):** Intervals are too wide, which reduces their operational utility. The staffing model will schedule too conservatively. Consider reducing `interval_width` or investigating whether outlier removal in preprocessing was too aggressive.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: DEPLOY AND MONITOR\n",
    "\n",
    "### 5.1 Hyperparameter Tuning\n",
    "\n",
    "Before deploying, systematically tune the key parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Grid search over Prophet's most impactful parameters\n",
    "# ──────────────────────────────────────────────────\n",
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.01, 0.05, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [1.0, 5.0, 10.0],\n",
    "    'holidays_prior_scale': [5.0, 10.0, 20.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "print(f'Testing {len(all_params)} parameter combinations...')\n",
    "\n",
    "results = []\n",
    "for i, params in enumerate(all_params):\n",
    "    m = Prophet(\n",
    "        growth='linear',\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        holidays=holidays_final,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    # Add regressors\n",
    "    for reg in ['is_direct_debit', 'is_payday_window', 'is_month_end',\n",
    "                'is_january', 'is_tax_season', 'is_weekend']:\n",
    "        m.add_regressor(reg, mode=params['seasonality_mode'])\n",
    "\n",
    "    m.fit(train)\n",
    "\n",
    "    cv = cross_validation(m, initial='730 days', period='7 days', horizon='7 days', parallel='processes')\n",
    "    metrics = performance_metrics(cv, rolling_window=1)\n",
    "\n",
    "    result = {**params,\n",
    "              'mae': metrics['mae'].mean(),\n",
    "              'mape': metrics['mape'].mean(),\n",
    "              'rmse': metrics['rmse'].mean(),\n",
    "              'coverage': metrics['coverage'].mean()}\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'  Completed {i + 1}/{len(all_params)}')\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('mape')\n",
    "print('\\nTop 5 parameter combinations by MAPE:')\n",
    "print(results_df.head().to_string(index=False))\n",
    "\n",
    "best_params = results_df.iloc[0].to_dict()\n",
    "print(f'\\nBest: MAPE={best_params[\"mape\"]*100:.1f}%, '\n",
    "      f'changepoint_prior={best_params[\"changepoint_prior_scale\"]}, '\n",
    "      f'seasonality_mode={best_params[\"seasonality_mode\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Production Forecast Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# production_forecast.py\n",
    "# Run every Monday morning at 06:45\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "FORECAST_HORIZON = 7\n",
    "BEST_PARAMS = {\n",
    "    'changepoint_prior_scale': 0.05,\n",
    "    'seasonality_prior_scale': 10.0,\n",
    "    'holidays_prior_scale': 10.0,\n",
    "    'seasonality_mode': 'additive',\n",
    "}\n",
    "\n",
    "def load_latest_data():\n",
    "    \"\"\"Pull latest call volume data from the ACD/CRM system.\"\"\"\n",
    "    # In production, this queries your data warehouse\n",
    "    df = pd.read_csv('daily_call_volume.csv', parse_dates=['date'])\n",
    "    df = df.rename(columns={'date': 'ds', 'calls': 'y'})\n",
    "    return df\n",
    "\n",
    "def data_quality_checks(df):\n",
    "    \"\"\"Validate the data before modelling.\"\"\"\n",
    "    checks = {}\n",
    "\n",
    "    # Check for missing recent days\n",
    "    expected_last_date = pd.Timestamp.today().normalize() - pd.Timedelta(days=1)\n",
    "    actual_last_date = df['ds'].max()\n",
    "    checks['last_date'] = actual_last_date\n",
    "    checks['data_current'] = actual_last_date >= expected_last_date - pd.Timedelta(days=2)\n",
    "\n",
    "    # Check for implausible values\n",
    "    recent = df[df['ds'] >= actual_last_date - pd.Timedelta(days=7)]\n",
    "    checks['recent_mean'] = recent['y'].mean()\n",
    "    checks['any_negative'] = (recent['y'] < 0).any()\n",
    "    checks['any_extreme'] = (recent['y'] > df['y'].quantile(0.999) * 2).any()\n",
    "\n",
    "    # Check for missing values\n",
    "    checks['missing_last_7d'] = recent['y'].isna().sum()\n",
    "\n",
    "    for check, value in checks.items():\n",
    "        logger.info(f'  {check}: {value}')\n",
    "\n",
    "    if checks['any_negative'] or checks['any_extreme'] or not checks['data_current']:\n",
    "        raise ValueError(f'Data quality check FAILED: {checks}')\n",
    "\n",
    "    return checks\n",
    "\n",
    "def build_and_forecast(df, holidays_df, horizon=FORECAST_HORIZON):\n",
    "    \"\"\"Fit Prophet and generate forecast.\"\"\"\n",
    "    model = Prophet(\n",
    "        growth='linear',\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        holidays=holidays_df,\n",
    "        interval_width=0.80,\n",
    "        **BEST_PARAMS\n",
    "    )\n",
    "\n",
    "    for reg in ['is_direct_debit', 'is_payday_window', 'is_month_end',\n",
    "                'is_january', 'is_tax_season', 'is_weekend']:\n",
    "        model.add_regressor(reg, mode=BEST_PARAMS['seasonality_mode'])\n",
    "\n",
    "    df = add_banking_regressors(df)\n",
    "    model.fit(df)\n",
    "\n",
    "    future = model.make_future_dataframe(periods=horizon, freq='D')\n",
    "    future = add_banking_regressors(future)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Extract forecast period only\n",
    "    result = forecast[forecast['ds'] > df['ds'].max()][['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()\n",
    "\n",
    "    # Floor at zero (call volume cannot be negative)\n",
    "    result['yhat'] = result['yhat'].clip(lower=0)\n",
    "    result['yhat_lower'] = result['yhat_lower'].clip(lower=0)\n",
    "\n",
    "    return model, result\n",
    "\n",
    "def sanity_check_forecast(forecast_df, df):\n",
    "    \"\"\"Verify the forecast is reasonable before publishing.\"\"\"\n",
    "    historical_mean = df['y'].tail(90).mean()\n",
    "    forecast_mean = forecast_df['yhat'].mean()\n",
    "\n",
    "    checks = {\n",
    "        'forecast_mean': forecast_mean,\n",
    "        'historical_90d_mean': historical_mean,\n",
    "        'ratio': forecast_mean / historical_mean,\n",
    "        'any_negative': (forecast_df['yhat'] < 0).any(),\n",
    "        'all_same': forecast_df['yhat'].std() < 1,\n",
    "    }\n",
    "\n",
    "    # Forecast should be within 50% of recent average\n",
    "    if not (0.5 < checks['ratio'] < 1.5):\n",
    "        raise ValueError(f'Forecast seems implausible: ratio={checks[\"ratio\"]:.2f}')\n",
    "\n",
    "    if checks['all_same']:\n",
    "        raise ValueError('Forecast is suspiciously flat')\n",
    "\n",
    "    logger.info(f'  Forecast sanity check PASSED: ratio={checks[\"ratio\"]:.2f}')\n",
    "    return checks\n",
    "\n",
    "def generate_staffing_inputs(forecast_df, aht_seconds=300, service_level=0.80,\n",
    "                             target_asa=20, shrinkage=0.30):\n",
    "    \"\"\"Convert daily forecast to staffing requirements via Erlang C.\"\"\"\n",
    "    from math import factorial\n",
    "\n",
    "    def erlang_c_agents(calls_per_day, aht, intervals_per_day=34):\n",
    "        \"\"\"Calculate agents needed per interval assuming uniform distribution.\n",
    "           In production, use the actual intraday arrival profile.\"\"\"\n",
    "        calls_per_interval = calls_per_day / intervals_per_day\n",
    "        interval_seconds = (12 * 3600) / intervals_per_day  # ~12 operating hours\n",
    "        traffic = (calls_per_interval * aht) / interval_seconds\n",
    "\n",
    "        for n in range(max(1, int(traffic) + 1), int(traffic) + 200):\n",
    "            sum_terms = sum((traffic ** k) / factorial(k) for k in range(n))\n",
    "            last_term = (traffic ** n) / factorial(n) * (n / (n - traffic))\n",
    "            pw = last_term / (sum_terms + last_term)\n",
    "            sl = 1 - pw * np.exp(-(n - traffic) * target_asa / aht)\n",
    "            if sl >= service_level:\n",
    "                return n\n",
    "        return n\n",
    "\n",
    "    staffing = forecast_df.copy()\n",
    "    staffing['raw_agents'] = staffing['yhat'].apply(lambda x: erlang_c_agents(x, aht_seconds))\n",
    "    staffing['scheduled_agents'] = np.ceil(staffing['raw_agents'] / (1 - shrinkage)).astype(int)\n",
    "    staffing['day'] = staffing['ds'].dt.day_name()\n",
    "\n",
    "    return staffing[['ds', 'day', 'yhat', 'yhat_lower', 'yhat_upper', 'raw_agents', 'scheduled_agents']]\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# Main execution\n",
    "# ──────────────────────────────────────────────────\n",
    "if __name__ == '__main__':\n",
    "    logger.info('=== WEEKLY FORECAST PIPELINE STARTED ===')\n",
    "\n",
    "    # Step 1: Load data\n",
    "    logger.info('Loading data...')\n",
    "    df = load_latest_data()\n",
    "\n",
    "    # Step 2: Quality checks\n",
    "    logger.info('Running data quality checks...')\n",
    "    data_quality_checks(df)\n",
    "\n",
    "    # Step 3: Build forecast\n",
    "    logger.info('Fitting Prophet and generating 7-day forecast...')\n",
    "    holidays_df = get_irish_bank_holidays(df['ds'].dt.year.min(), df['ds'].dt.year.max() + 1)\n",
    "    model, forecast_7d = build_and_forecast(df, holidays_df)\n",
    "\n",
    "    # Step 4: Sanity check\n",
    "    logger.info('Sanity checking forecast...')\n",
    "    sanity_check_forecast(forecast_7d, df)\n",
    "\n",
    "    # Step 5: Generate staffing plan\n",
    "    logger.info('Generating staffing requirements...')\n",
    "    staffing = generate_staffing_inputs(forecast_7d)\n",
    "\n",
    "    print('\\n' + '=' * 80)\n",
    "    print('7-DAY CALL VOLUME FORECAST AND STAFFING PLAN')\n",
    "    print('=' * 80)\n",
    "    print(f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "    print(f'Forecast period: {forecast_7d[\"ds\"].min().strftime(\"%Y-%m-%d\")} to '\n",
    "          f'{forecast_7d[\"ds\"].max().strftime(\"%Y-%m-%d\")}')\n",
    "    print('-' * 80)\n",
    "    print(f'{\"Date\":<12} {\"Day\":<10} {\"Forecast\":>9} {\"Low 80%\":>9} {\"High 80%\":>9} '\n",
    "          f'{\"Agents\":>8} {\"Scheduled\":>10}')\n",
    "    print('-' * 80)\n",
    "    for _, row in staffing.iterrows():\n",
    "        print(f'{row[\"ds\"].strftime(\"%Y-%m-%d\"):<12} {row[\"day\"]:<10} '\n",
    "              f'{row[\"yhat\"]:>9,.0f} {row[\"yhat_lower\"]:>9,.0f} {row[\"yhat_upper\"]:>9,.0f} '\n",
    "              f'{row[\"raw_agents\"]:>8d} {row[\"scheduled_agents\"]:>10d}')\n",
    "    print('-' * 80)\n",
    "    print(f'{\"TOTAL\":<22} {staffing[\"yhat\"].sum():>9,.0f} {staffing[\"yhat_lower\"].sum():>9,.0f} '\n",
    "          f'{staffing[\"yhat_upper\"].sum():>9,.0f}')\n",
    "    print('=' * 80)\n",
    "\n",
    "    # Step 6: Export\n",
    "    staffing.to_csv(f'forecast_{datetime.now().strftime(\"%Y%m%d\")}.csv', index=False)\n",
    "    logger.info(f'Forecast exported to forecast_{datetime.now().strftime(\"%Y%m%d\")}.csv')\n",
    "    logger.info('=== PIPELINE COMPLETE ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Weekly Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# monitor_forecast.py\n",
    "# Run every Monday AFTER actuals for last week are available\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "def weekly_accuracy_report(actuals_df, forecast_df):\n",
    "    \"\"\"\n",
    "    Compare last week's forecast against actuals.\n",
    "    \n",
    "    Parameters:\n",
    "        actuals_df: DataFrame with 'ds' and 'y' (actual call volumes)\n",
    "        forecast_df: DataFrame with 'ds', 'yhat', 'yhat_lower', 'yhat_upper'\n",
    "    \"\"\"\n",
    "    merged = actuals_df.merge(forecast_df, on='ds', how='inner')\n",
    "    merged['error'] = merged['y'] - merged['yhat']\n",
    "    merged['abs_error'] = merged['error'].abs()\n",
    "    merged['pct_error'] = merged['error'] / merged['y'] * 100\n",
    "    merged['abs_pct_error'] = merged['pct_error'].abs()\n",
    "    merged['within_interval'] = (merged['y'] >= merged['yhat_lower']) & (merged['y'] <= merged['yhat_upper'])\n",
    "    merged['day'] = merged['ds'].dt.day_name()\n",
    "\n",
    "    # Summary metrics\n",
    "    report = {\n",
    "        'week_ending': merged['ds'].max(),\n",
    "        'mae': merged['abs_error'].mean(),\n",
    "        'mape': merged['abs_pct_error'].mean(),\n",
    "        'rmse': np.sqrt((merged['error'] ** 2).mean()),\n",
    "        'bias': merged['error'].mean(),\n",
    "        'coverage': merged['within_interval'].mean() * 100,\n",
    "        'worst_day': merged.loc[merged['abs_pct_error'].idxmax(), 'day'],\n",
    "        'worst_day_error': merged['abs_pct_error'].max(),\n",
    "    }\n",
    "\n",
    "    # Print report\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'FORECAST ACCURACY REPORT — Week Ending {report[\"week_ending\"].strftime(\"%Y-%m-%d\")}')\n",
    "    print('=' * 70)\n",
    "    print(f'  MAE       : {report[\"mae\"]:>8,.0f} calls')\n",
    "    print(f'  MAPE      : {report[\"mape\"]:>8.1f}%')\n",
    "    print(f'  RMSE      : {report[\"rmse\"]:>8,.0f} calls')\n",
    "    print(f'  Bias      : {report[\"bias\"]:>+8,.0f} calls (positive = underprediction)')\n",
    "    print(f'  Coverage  : {report[\"coverage\"]:>8.1f}% (target: 80%)')\n",
    "    print(f'  Worst day : {report[\"worst_day\"]} ({report[\"worst_day_error\"]:.1f}% error)')\n",
    "    print()\n",
    "\n",
    "    # Day-by-day breakdown\n",
    "    print(f'{\"Date\":<12} {\"Day\":<10} {\"Actual\":>8} {\"Forecast\":>9} {\"Error\":>8} {\"%Error\":>8} {\"In CI?\":>7}')\n",
    "    print('-' * 70)\n",
    "    for _, row in merged.iterrows():\n",
    "        ci_flag = '✅' if row['within_interval'] else '❌'\n",
    "        print(f'{row[\"ds\"].strftime(\"%Y-%m-%d\"):<12} {row[\"day\"]:<10} '\n",
    "              f'{row[\"y\"]:>8,.0f} {row[\"yhat\"]:>9,.0f} {row[\"error\"]:>+8,.0f} '\n",
    "              f'{row[\"pct_error\"]:>+7.1f}% {ci_flag:>7}')\n",
    "\n",
    "    # Alerts\n",
    "    print()\n",
    "    if report['mape'] > 15:\n",
    "        print('🚨 ALERT: MAPE exceeds 15% — investigate and consider retraining')\n",
    "    elif report['mape'] > 10:\n",
    "        print('⚠️  WARNING: MAPE above 10% — monitor closely next week')\n",
    "    else:\n",
    "        print('✅ Performance within acceptable range')\n",
    "\n",
    "    if abs(report['bias']) > 0.08 * merged['y'].mean():\n",
    "        direction = 'underpredicting' if report['bias'] > 0 else 'overpredicting'\n",
    "        print(f'⚠️  BIAS ALERT: Model is systematically {direction} '\n",
    "              f'by {abs(report[\"bias\"]):.0f} calls/day')\n",
    "\n",
    "    if report['coverage'] < 65:\n",
    "        print('⚠️  COVERAGE ALERT: Prediction intervals are too narrow — '\n",
    "              f'{report[\"coverage\"]:.0f}% vs 80% target')\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Retraining Schedule and Drift Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# Retraining decision logic\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "def should_retrain(performance_history, weeks_since_retrain):\n",
    "    \"\"\"\n",
    "    Decide whether to retrain the model based on recent performance.\n",
    "\n",
    "    Returns: (should_retrain: bool, reason: str)\n",
    "    \"\"\"\n",
    "    if len(performance_history) < 4:\n",
    "        return False, 'Insufficient history for drift detection'\n",
    "\n",
    "    recent = performance_history.tail(4)\n",
    "\n",
    "    # Trigger 1: Rolling MAPE above threshold\n",
    "    rolling_mape = recent['mape'].mean()\n",
    "    if rolling_mape > 12:\n",
    "        return True, f'Rolling 4-week MAPE = {rolling_mape:.1f}% (threshold: 12%)'\n",
    "\n",
    "    # Trigger 2: Systematic bias\n",
    "    rolling_bias = recent['bias'].mean()\n",
    "    avg_volume = recent['mae'].mean() / (recent['mape'].mean() / 100)  # Approximate\n",
    "    if abs(rolling_bias) > 0.08 * avg_volume:\n",
    "        return True, f'Systematic bias = {rolling_bias:+.0f} calls/day'\n",
    "\n",
    "    # Trigger 3: Time-based (retrain at least monthly)\n",
    "    if weeks_since_retrain >= 4:\n",
    "        return True, f'{weeks_since_retrain} weeks since last retrain (threshold: 4)'\n",
    "\n",
    "    # Trigger 4: Coverage degradation\n",
    "    rolling_coverage = recent['coverage'].mean()\n",
    "    if rolling_coverage < 65:\n",
    "        return True, f'Coverage = {rolling_coverage:.0f}% (threshold: 65%)'\n",
    "\n",
    "    return False, f'Performance OK — MAPE={rolling_mape:.1f}%, Bias={rolling_bias:+.0f}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Fallback Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_with_fallback(df, holidays_df, horizon=7):\n",
    "    \"\"\"\n",
    "    Layered forecasting with automatic fallback.\n",
    "    \n",
    "    Level 1: Prophet with tuned parameters and regressors\n",
    "    Level 2: Prophet with default parameters (more robust)\n",
    "    Level 3: 4-week same-day-of-week average (always works)\n",
    "    \"\"\"\n",
    "\n",
    "    # Level 1: Full Prophet model\n",
    "    try:\n",
    "        logger.info('Attempting Level 1: Tuned Prophet...')\n",
    "        model, forecast = build_and_forecast(df, holidays_df, horizon)\n",
    "        sanity_check_forecast(forecast, df)\n",
    "        logger.info('Level 1 SUCCESS')\n",
    "        return forecast, 'prophet_tuned'\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Level 1 FAILED: {e}')\n",
    "\n",
    "    # Level 2: Simple Prophet (no custom regressors)\n",
    "    try:\n",
    "        logger.info('Attempting Level 2: Default Prophet...')\n",
    "        simple_model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            holidays=holidays_df\n",
    "        )\n",
    "        simple_model.add_country_holidays(country_name='IE')\n",
    "        simple_df = df[['ds', 'y']].copy()\n",
    "        simple_model.fit(simple_df)\n",
    "\n",
    "        future = simple_model.make_future_dataframe(periods=horizon)\n",
    "        forecast = simple_model.predict(future)\n",
    "        forecast = forecast[forecast['ds'] > df['ds'].max()][['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "        forecast['yhat'] = forecast['yhat'].clip(lower=0)\n",
    "\n",
    "        logger.info('Level 2 SUCCESS')\n",
    "        return forecast, 'prophet_simple'\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Level 2 FAILED: {e}')\n",
    "\n",
    "    # Level 3: Naïve 4-week average (always works)\n",
    "    logger.info('Falling back to Level 3: 4-week same-day average...')\n",
    "    df_indexed = df.set_index('ds')\n",
    "\n",
    "    forecast_dates = pd.date_range(df['ds'].max() + pd.Timedelta(days=1), periods=horizon, freq='D')\n",
    "    naive_forecasts = []\n",
    "\n",
    "    for date in forecast_dates:\n",
    "        same_dow_dates = [date - pd.Timedelta(weeks=w) for w in range(1, 5)]\n",
    "        vals = df_indexed.loc[df_indexed.index.isin(same_dow_dates), 'y']\n",
    "        mean_val = vals.mean() if len(vals) > 0 else df['y'].tail(28).mean()\n",
    "        std_val = vals.std() if len(vals) > 1 else df['y'].tail(28).std()\n",
    "\n",
    "        naive_forecasts.append({\n",
    "            'ds': date,\n",
    "            'yhat': mean_val,\n",
    "            'yhat_lower': mean_val - 1.28 * std_val,  # ~80% CI\n",
    "            'yhat_upper': mean_val + 1.28 * std_val\n",
    "        })\n",
    "\n",
    "    forecast = pd.DataFrame(naive_forecasts)\n",
    "    logger.info('Level 3 SUCCESS (naive fallback)')\n",
    "    return forecast, 'naive_4week'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet-Specific Tuning Guide\n",
    "\n",
    "| Parameter | What It Controls | Default | Tune When... | Range |\n",
    "|---|---|---|---|---|\n",
    "| `changepoint_prior_scale` | Trend flexibility | 0.05 | Trend looks too rigid or too wiggly | 0.001 – 0.5 |\n",
    "| `seasonality_prior_scale` | Seasonal smoothness | 10 | Seasonal pattern is too smooth or overfitting | 0.01 – 25 |\n",
    "| `holidays_prior_scale` | Holiday effect size | 10 | Holiday spikes under/overestimated | 0.01 – 25 |\n",
    "| `seasonality_mode` | Additive vs multiplicative | additive | Seasonal amplitude varies with level | additive / multiplicative |\n",
    "| `changepoint_range` | Where changepoints can occur | 0.8 | Recent trend shifts are important | 0.7 – 0.95 |\n",
    "| `n_changepoints` | Number of potential changepoints | 25 | Trend has many/few level shifts | 10 – 50 |\n",
    "| `yearly_seasonality` | Fourier order for annual cycle | auto (10) | Annual pattern is too smooth/jagged | 3 – 25 |\n",
    "| `weekly_seasonality` | Fourier order for weekly cycle | auto (3) | Day-of-week pattern not captured | 3 – 10 |\n",
    "| `interval_width` | Prediction interval width | 0.80 | Calibration is poor | 0.70 – 0.95 |\n",
    "| `mcmc_samples` | Bayesian sampling for uncertainty | 0 (MAP) | PI calibration matters for staffing | 0 or 300+ |\n",
    "\n",
    "**Tuning priority order for call volume:** `seasonality_mode` → `changepoint_prior_scale` → `holidays_prior_scale` → `weekly_seasonality` Fourier order → everything else.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known Prophet Limitations and Workarounds\n",
    "\n",
    "| Limitation | Impact on Call Forecasting | Workaround |\n",
    "|---|---|---|\n",
    "| No autocorrelation modelling | Cannot capture \"bad week carries into next week\" momentum | Post-process residuals with AR(1), or ensemble with XGBoost |\n",
    "| Sensitive to outliers in trend estimation | Single extreme day can create a false changepoint | Set outliers to NaN before fitting |\n",
    "| Holiday effects are constant over time | A bank holiday in 2020 (COVID) had a different effect than in 2024 | Use recent-only training data, or add interaction regressors |\n",
    "| Cannot model intermittent demand | If weekends have zero volume, Prophet may produce negative forecasts for weekends | Clip forecasts at zero; or model weekdays and weekends separately |\n",
    "| Annual seasonality needs > 2 years of data | Fourier terms cannot be estimated with < 2 full annual cycles | Use `yearly_seasonality=False` and model annual effects via monthly regressors if short history |\n",
    "| No built-in ensemble or model averaging | Prophet gives one forecast; no automatic model combination | Build your own ensemble: 60% Prophet + 40% XGBoost, or use prediction interval overlap as a confidence measure |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference — Complete Pipeline Checklist\n",
    "\n",
    "```\n",
    "□ EXPLORE\n",
    "  □ Plot full history — identify trend, seasonality, anomalies\n",
    "  □ Day-of-week box plots — confirm weekly pattern\n",
    "  □ Monthly/annual patterns — identify secondary seasonality\n",
    "  □ MSTL decomposition (periods 7 and 365)\n",
    "  □ ACF/PACF — confirm lag 7 dominance\n",
    "  □ Stationarity tests (ADF + KPSS) — informational for Prophet\n",
    "\n",
    "□ PREPROCESS\n",
    "  □ Create Prophet df with ds/y columns\n",
    "  □ Handle missing days (interpolate or fill)\n",
    "  □ Flag and NaN true outliers (keep holiday effects)\n",
    "  □ Build holiday DataFrame with lower/upper windows\n",
    "  □ Build banking regressors (direct debit, payday, weekend)\n",
    "  □ Train/test split (hold out 8+ weeks)\n",
    "\n",
    "□ MODEL\n",
    "  □ Compute seasonal naïve baseline (MAPE target to beat)\n",
    "  □ Configure Prophet (growth, seasonalities, mode)\n",
    "  □ Add holidays and regressors\n",
    "  □ Fit and inspect components\n",
    "  □ Check changepoints — do they make sense?\n",
    "  □ Generate 7-day forecast with prediction intervals\n",
    "\n",
    "□ DIAGNOSE\n",
    "  □ Residual time plot — no pattern\n",
    "  □ Residual ACF — no significant spikes\n",
    "  □ Residuals by day of week — no systematic bias\n",
    "  □ Ljung-Box test — p > 0.05\n",
    "  □ Prophet cross-validation (initial/period/horizon)\n",
    "  □ MAPE by horizon — stable or degrading?\n",
    "  □ MAPE by day of week — Monday worst?\n",
    "  □ Prediction interval coverage — near 80%?\n",
    "\n",
    "□ DEPLOY & MONITOR\n",
    "  □ Hyperparameter grid search via cross-validation\n",
    "  □ Production script with data quality checks\n",
    "  □ Sanity checks on forecast before publishing\n",
    "  □ Erlang C integration for staffing output\n",
    "  □ Weekly accuracy report (MAE, MAPE, bias, coverage)\n",
    "  □ Drift detection and retraining triggers\n",
    "  □ 3-level fallback (tuned → simple → naïve)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Case study prepared as a supplement to the Time Series Fundamentals and EDA study notes. Focused on Prophet for the workforce planning function at a retail bank contact centre. February 2026.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
